#!/usr/bin/env python3
# -*- coding: utf-8 -*-

""" それぞれがDQNを保有する群ロボットの回避行動の獲得 """

import rospy
import os
import numpy as np
import pandas as pd
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from datetime import datetime
import torch.nn.functional as F # 活性化関数
import sqlite3
# 強化学習ライブラリ
import pfrl
import torch
# 使用自作ファイル
from env_goal_3rb_sac import Env # 環境ファイル
import net_goal_3rb_sac # ネットワーク
import greedy_goal_3rb_sac # 探索手法

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

import os
import torch
import pfrl
import net_goal_3rb_sac  # net_goal_3rb_sacのインポート

class ReinforceAgent():
    def __init__(self):
        # モデルを保存するためのパス
        self.current_dirPath = os.path.dirname(os.path.realpath(__file__))  # カレントディレクトリのパス
        self.dirPath = self.current_dirPath.replace('/nodes', '/save_model')
        
        self.gpu = 0  # GPUを使用
        
        # アクターネットワーク（ポリシーネットワーク）
        self.policy = net_goal_3rb_sac.PolicyNetwork(
            state_dim=3,
            action_dim=3
        )
        
        # クリティックネットワーク（Q関数）
        self.q_func1 = net_goal_3rb_sac.QFunction(
            state_dim=3,
            action_dim=3
        )
        
        self.q_func2 = net_goal_3rb_sac.QFunction(
            state_dim=3,
            action_dim=3
        )
        
        # 最適化アルゴリズム
        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)
        self.q_func1_optimizer = torch.optim.Adam(self.q_func1.parameters(), lr=3e-4)
        self.q_func2_optimizer = torch.optim.Adam(self.q_func2.parameters(), lr=3e-4)
        
        # リプレイバッファ
        self.rbuf = pfrl.replay_buffers.ReplayBuffer(capacity=10**6)  # ReplayBufferのインスタンス作成
        
        # SACエージェントの作成
        agent = pfrl.agents.SoftActorCritic(
            policy=self.policy,  # 定義したpolicyモデルを渡します
            q_func1=self.q_func1,  # 定義したq_func1を渡します
            q_func2=self.q_func2,  # 定義したq_func2を渡します
            policy_optimizer=self.policy_optimizer,  # ポリシーの最適化アルゴリズム
            q_func1_optimizer=self.q_func1_optimizer,  # Q関数1の最適化アルゴリズム
            q_func2_optimizer=self.q_func2_optimizer,  # Q関数2の最適化アルゴリズム
            replay_buffer=self.rbuf,  # ReplayBufferを渡します
            gamma=0.99,  # 割引率
            replay_start_size=10000,  # リプレイバッファの開始サイズ
            minibatch_size=100,  # ミニバッチサイズ
            update_interval=1,  # 更新間隔
            phi=lambda x: x,  # 状態空間変換関数（必要に応じて変更）
            soft_update_tau=5e-3,  # ソフトアップデートのタウ
            max_grad_norm=None,  # 勾配クリッピングの最大ノルム
            logger=None,  # ロガー（必要に応じて設定）
            batch_states=None,  # 状態バッチの設定
            burnin_action_func=None,  # アクション関数（必要に応じて設定）
            initial_temperature=1.0,  # 初期温度
            entropy_target=None,  # エントロピーターゲット（必要に応じて設定）
            temperature_optimizer_lr=None,  # 温度最適化の学習率
            act_deterministically=True,  # 決定的に行動するかどうか
        )


# メイン
if __name__ == '__main__':
    rospy.init_node('main_goal_3rb')
    agent = ReinforceAgent()