#!/usr/bin/env python3
# -*- coding: utf-8 -*-

""" それぞれがDQNを保有する群ロボットの回避行動の獲得 """

import rospy
import os
import numpy as np
import pandas as pd
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from datetime import datetime
import torch.nn.functional as F # 活性化関数
import sqlite3
import math
# 強化学習ライブラリ
import pfrl
import torch
# 使用自作ファイル
from env_goal_3rb_sac import Env # 環境ファイル
import net_goal_3rb_sac # ネットワーク
from pfrl.utils import clip_l2_grad_norm_
from pfrl.agents import SoftActorCritic

class CustomSoftActorCritic(SoftActorCritic):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def update_q_func(self, batch):
        """Compute loss for a given Q-function."""

        batch_next_state = batch["next_state"]
        batch_rewards = batch["reward"]
        batch_terminal = batch["is_state_terminal"]
        batch_state = batch["state"]
        batch_actions = batch["action"]
        batch_discount = batch["discount"]

        with torch.no_grad(), pfrl.utils.evaluating(self.policy), pfrl.utils.evaluating(
            self.target_q_func1
        ), pfrl.utils.evaluating(self.target_q_func2):
            # 次状態からアクション分布を取得
            next_action_distrib = self.policy(batch_next_state)

            # サンプルされたアクション
            next_actions = next_action_distrib.sample()

            # ログ確率を計算
            next_log_prob = next_action_distrib.log_prob(next_actions)

            # 次状態でのQ値を計算
            next_q1 = self.target_q_func1((batch_next_state, next_actions))
            next_q2 = self.target_q_func2((batch_next_state, next_actions))
            next_q = torch.min(next_q1, next_q2)

            # エントロピー項を計算し形状を調整
            next_log_prob = next_log_prob.mean(dim=1, keepdim=True)  # アクション次元を平均化
            entropy_term = self.temperature * next_log_prob  # 形状: [64, 1]

            # 確認
            assert next_q.shape == entropy_term.shape

            target_q = batch_rewards + batch_discount * (
                1.0 - batch_terminal
            ) * torch.flatten(next_q - entropy_term)

        predict_q1 = torch.flatten(self.q_func1((batch_state, batch_actions)))
        predict_q2 = torch.flatten(self.q_func2((batch_state, batch_actions)))

        loss1 = 0.5 * F.mse_loss(target_q, predict_q1)
        loss2 = 0.5 * F.mse_loss(target_q, predict_q2)

        # Update stats
        self.q1_record.extend(predict_q1.detach().cpu().numpy())
        self.q2_record.extend(predict_q2.detach().cpu().numpy())
        self.q_func1_loss_record.append(float(loss1))
        self.q_func2_loss_record.append(float(loss2))

        self.q_func1_optimizer.zero_grad()
        loss1.backward()
        if self.max_grad_norm is not None:
            clip_l2_grad_norm_(self.q_func1.parameters(), self.max_grad_norm)
        self.q_func1_optimizer.step()

        self.q_func2_optimizer.zero_grad()
        loss2.backward()
        if self.max_grad_norm is not None:
            clip_l2_grad_norm_(self.q_func2.parameters(), self.max_grad_norm)
        self.q_func2_optimizer.step()
    
    def update_policy_and_temperature(self, batch):
        """Compute loss for actor."""

        batch_state = batch["state"]

        # 現在の状態でのアクション分布を計算
        action_distrib = self.policy(batch_state)
        sampled_actions = action_distrib.sample()
        log_prob = action_distrib.log_prob(sampled_actions)

        # Q値の計算
        q1 = self.q_func1((batch_state, sampled_actions))
        q2 = self.q_func2((batch_state, sampled_actions))
        q = torch.min(q1, q2)  # 最小値を使用

        # エントロピー項の計算
        entropy_term = self.temperature * log_prob.mean(dim=1, keepdim=True)

        # 確認
        assert q.shape == entropy_term.shape, f"q.shape={q.shape}, entropy_term.shape={entropy_term.shape}"
        loss = torch.mean(entropy_term - q)

        self.policy_optimizer.zero_grad()
        loss.backward()
        if self.max_grad_norm is not None:
            clip_l2_grad_norm_(self.policy.parameters(), self.max_grad_norm)
        self.policy_optimizer.step()

        self.n_policy_updates += 1

        if self.entropy_target is not None:
            self.update_temperature(log_prob.detach())

        # Record entropy
        with torch.no_grad():
            try:
                self.entropy_record.extend(
                    action_distrib.entropy().detach().cpu().numpy()
                )
            except NotImplementedError:
                # Record - log p(x) instead
                self.entropy_record.extend(-log_prob.detach().cpu().numpy())

class ReinforceAgent():
    def __init__(self):
        # モデルを保存するためのパス
        self.current_dirPath = os.path.dirname(os.path.realpath(__file__))  # カレントディレクトリのパス
        self.dirPath = self.current_dirPath.replace('/nodes', '/save_model')
        
        self.mode = 'sim'
        self.load_model = False
        self.load_rbuf = False
        self.episode = 30
        self.episode_step = 500
        self.lidar_num = 72
        self.save_rbuf = False
        self.recovery = False
        self.screen_list = [0]
        self.gpu = 0  # GPUを使用する場合は0に設定、GPUがない場合は-1
        action_dim = 2  # 直進速度と回転速度の2つの出力
        action_space = np.array([-1.0, 1.0, -math.pi, math.pi])

        self.robot_n = rospy.get_param('robot_num')
        
        # GPUを使用するかどうかを決定
        device = torch.device('cuda' if torch.cuda.is_available() and self.gpu != -1 else 'cpu')
        
        # アクターネットワーク（ポリシーネットワーク）
        self.policy = net_goal_3rb_sac.PolicyNetwork(
            state_dim=3,
            action_dim=action_dim
        ).to(device)  # GPUデバイスにモデルを転送
        
        # クリティックネットワーク（Q関数）
        self.q_func1 = net_goal_3rb_sac.QFunction(
            state_dim=3,
            action_dim=action_dim
        ).to(device)  # GPUデバイスにモデルを転送
        
        self.q_func2 = net_goal_3rb_sac.QFunction(
            state_dim=3,
            action_dim=action_dim
        ).to(device)  # GPUデバイスにモデルを転送
        
        # 最適化アルゴリズム
        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=3e-4)
        self.q_func1_optimizer = torch.optim.Adam(self.q_func1.parameters(), lr=3e-4)
        self.q_func2_optimizer = torch.optim.Adam(self.q_func2.parameters(), lr=3e-4)
        
        # リプレイバッファ
        self.rbuf = pfrl.replay_buffers.ReplayBuffer(capacity=10**6)  # ReplayBufferのインスタンス作成
        
        # SACエージェントの作成
        self.model = CustomSoftActorCritic(
            policy=self.policy,  # 定義したpolicyモデルを渡します
            q_func1=self.q_func1,  # 定義したq_func1を渡します
            q_func2=self.q_func2,  # 定義したq_func2を渡します
            policy_optimizer=self.policy_optimizer,  # ポリシーの最適化アルゴリズム
            q_func1_optimizer=self.q_func1_optimizer,  # Q関数1の最適化アルゴリズム
            q_func2_optimizer=self.q_func2_optimizer,  # Q関数2の最適化アルゴリズム
            replay_buffer=self.rbuf,  # ReplayBufferを渡します
            gamma=0.99,  # 割引率
            replay_start_size=10000,  # リプレイバッファの開始サイズ
            minibatch_size=100,  # ミニバッチサイズ
            update_interval=1,  # 更新間隔
            phi=lambda x: x,  # 状態空間変換関数（必要に応じて変更）
            soft_update_tau=5e-3,  # ソフトアップデートのタウ
            max_grad_norm=None,  # 勾配クリッピングの最大ノルム
            logger=None,  # ロガー（必要に応じて設定）
            batch_states=None,  # 状態バッチの設定
            burnin_action_func=None,  # アクション関数（必要に応じて設定）
            initial_temperature=1.0,  # 初期温度
            entropy_target=None,  # エントロピーターゲット（必要に応じて設定）
            temperature_optimizer_lr=None,  # 温度最適化の学習率
            act_deterministically=True,  # 決定的に行動するかどうか
            gpu=self.gpu  # 使用するGPUを整数で指定
        )
    
    def train(self):

        # 使用するモデルのロード
        if self.load_model:
            self.model.load(self.dirPath + '/load_model/robot_' + str(self.robot_n))
        if self.load_rbuf:
            self.rbuf.load(self.dirPath + '/load_model/robot_' + str(self.robot_n) + '/replay_buffer')
        
        start_time = time.time() # 学習開始時間の取得

        if self.mode == 'sim':
            env.set_robot(0) # 初期位置に配置

        if self.robot_n in self.screen_list:
            print('start train (robot' + str(self.robot_n) + ')')
        
        old_score = []
        old_collisions = []
        r_just_list = []
        color_list = []

        collision = False

        for e in range(1, self.episode + 1):

            state = env.reset()
            state = torch.from_numpy(state).float()

            score = 0
            collisions = 0
            color_count = 0
            r_just_count = 0

            turning_num = 0

            for t in range(1, self.episode_step + 1):
                
                action = self.model.act(state) # モデルに状態を渡して取るべき行動を算出

                ##### shielding(リカバリー方策) #####
                if self.recovery and t >= 2:
                    if color_num < 50:
                        action = env.recovery_change_action(e, self.lidar_num, action, state, self.model.model) # LiDARの数値が低い方向への行動を避ける
                    pass
                #####################################

                next_state, reward, color_num, just_count, collision, _ = env.step(action, test=False) # 行動、状態の観測、報酬の一連の処理
                next_state = torch.from_numpy(next_state).float()
                self.model.observe(next_state, reward, collision, reset=False) # モデルの更新
                self.model.update()

                r_just_count += just_count
                color_count += color_num
                score += reward
                state = next_state

                if action in [3, 4]:
                    turning_num += 1

                # 衝突時の処理
                if collision:
                    collisions += 1
                    collision = False
                
                # エピソードの最後の処理
                if t == self.episode_step:

                    # 秒、分、時間の取得
                    m, s = divmod(int(time.time() - start_time), 60)
                    h, m = divmod(m, 60)

                    # 結果の出力
                    if self.robot_n in self.screen_list:
                        print('Ep: {} score: {:.2f} collision: {} goal: {} time: {}:{:02d}:{:02d}'.format(
                              e, score, collisions, r_just_count, h, m, s))
                    
                    # 各結果をリストへ格納
                    old_score.append(score)
                    old_collisions.append(collisions)
                    r_just_list.append(r_just_count)
                    color_list.append(color_count)

                    break
            
            # モデルの保存
            if self.save_rbuf:
                self.rbuf.save(self.dirPath + '/TRIAL' + str(trials) + '/episode' + str(e) + '/robot_' + str(self.robot_n) + '/replay_buffer') # リプレイバッファの保存
            self.model.save(self.dirPath + '/TRIAL' + str(trials) + '/episode' + str(e) + '/robot_' + str(self.robot_n)) # モデルの保存

# メイン
if __name__ == '__main__':
    rospy.init_node('main_goal_3rb_sac')
    trials = 55
    agent = ReinforceAgent()
    env = Env()
    agent.train()